\begingroup

\begin{table}[!htbp]

\caption%
[Knowledge-based disambiguation results in the MSH WSD dataset using word embeddings.]%
{%
% \small%
% \fontsize{11pt}{13.2pt}\selectfont%
\fontsize{10.9pt}{13.08pt}\selectfont%
Knowledge-based disambiguation results in the MSH WSD dataset using word embeddings, \as{cui} textual definitions, and \as{cui}--\as{cui} association values. Different weighting schemes for averaging the word embeddings in the creation of the context vectors are compared. The evaluation metric is accuracy and the results are the average across five folds. Rows 1--6 present the results when only the cosine similarity between the ambiguous term' context vector and each candidate concept vector is considered. Rows 7--12, 13--18, and 19--24 additionally consider the cosine similarities of related concepts that have a $\NPMI$ value greater or equal than 0.8, 0.5, and 0.3 respectively. The highest accuracy, in each of these groups, is highlighted in bold. The overall highest accuracy is also underlined.}
\label{tab:wsd-kb}

\centering

% \small
\footnotesize
% \fontsize{10pt}{12pt}\selectfont
% \fontsize{9.9pt}{11.88pt}\selectfont
% \scriptsize

\begin{tabular}{G{0.4cm}D{2.1cm}G{0.5cm}G{0.4cm}G{1.18cm}G{1.18cm}G{1.18cm}G{1.18cm}G{1.18cm}}
\toprule

& \textbf{CUI--CUI} & \multicolumn{2}{l}{\textbf{WE}\textsuperscript{†}} & \multicolumn{5}{l}{\textbf{Weighting scheme}\textsuperscript{‡}}\\
\multicolumn{1}{l}{Row} & \textbf{associations}* & \multicolumn{1}{c}{S} & \multicolumn{1}{c}{W} & \multicolumn{1}{c}{TF} & \multicolumn{1}{c}{None} & \multicolumn{1}{c}{Frac.} & \multicolumn{1}{c}{Exp.} & \multicolumn{1}{c}{Log.}\\

\midrule

1 & CS & 100 &  5 & 0.8144 & 0.8164 & 0.8415 & 0.8259 & 0.8318\\
2 &    &     & 20 & 0.8254 & 0.8286 & 0.8473 & 0.8270 & 0.8407\\
3 &    &     & 50 & 0.8321 & 0.8341 & 0.8502 & 0.8278 & 0.8468\\[2pt]
4 &    & 300 &  5 & 0.8181 & 0.8203 & 0.8457 & 0.8278 & 0.8355\\
5 &    &     & 20 & 0.8319 & 0.8352 & \textbf{0.8533} & 0.8302 & 0.8477\\
6 &    &     & 50 & 0.8337 & 0.8365 & \textbf{0.8533} & 0.8276 & 0.8501\\

\midrule

 7 & $\NPMI \geq 0.8$ & 100 &  5 & 0.8132 & 0.8154 & 0.8395 & 0.8236 & 0.8304\\
 8 &                  &     & 20 & 0.8243 & 0.8277 & 0.8459 & 0.8255 & 0.8395\\
 9 &                  &     & 50 & 0.8314 & 0.8334 & 0.8493 & 0.8264 & 0.8461\\[2pt]
10 &                  & 300 &  5 & 0.8168 & 0.8193 & 0.8438 & 0.8255 & 0.8340\\
11 &                  &     & 20 & 0.8312 & 0.8343 & 0.8515 & 0.8283 & 0.8466\\
12 &                  &     & 50 & 0.8332 & 0.8357 & \textbf{0.8518} & 0.8264 & 0.8491\\

\midrule

13 & $\NPMI \geq 0.5$ & 100 &  5 & 0.8005 & 0.8019 & 0.8234 & 0.8057 & 0.8155\\
14 &                  &     & 20 & 0.8152 & 0.8178 & 0.8348 & 0.8137 & 0.8290\\
15 &                  &     & 50 & 0.8197 & 0.8236 & 0.8376 & 0.8150 & 0.8343\\[2pt]
16 &                  & 300 &  5 & 0.8030 & 0.8057 & 0.8267 & 0.8092 & 0.8190\\
17 &                  &     & 20 & 0.8174 & 0.8203 & 0.8377 & 0.8162 & 0.8323\\
18 &                  &     & 50 & 0.8209 & 0.8245 & \textbf{0.8396} & 0.8168 & 0.8352\\

\midrule

19 & $\NPMI \geq 0.3$ & 100 &  5 & 0.8430 & 0.8458 & 0.8617 & 0.8378 & 0.8560\\
20 &                  &     & 20 & 0.8573 & 0.8600 & 0.8720 & 0.8458 & 0.8704\\
21 &                  &     & 50 & 0.8600 & 0.8635 & \underline{\textbf{0.8744}} & 0.8459 & 0.8730\\[2pt]
22 &                  & 300 &  5 & 0.8446 & 0.8471 & 0.8622 & 0.8404 & 0.8573\\
23 &                  &     & 20 & 0.8566 & 0.8598 & 0.8730 & 0.8469 & 0.8705\\
24 &                  &     & 50 & 0.8582 & 0.8611 & 0.8736 & 0.8478 & 0.8719\\

\bottomrule

\multicolumn{9}{D{14.2cm}}{* CS: cosine similarity between the term context vector and each candidate concept vector only. $\NPMI \geq threshold$: concepts with a $\NPMI$ value, with respect to the candidate concept, greater or equal than the threshold are considered.}\\
\multicolumn{9}{D{14.2cm}}{\textsuperscript{†} Word embeddings model trained with a specific vector size (S) and context window (W).}\\
\multicolumn{9}{D{14.2cm}}{\textsuperscript{‡} Different weighting schemes are used to create the context embeddings. The IDF value is implicitly considered in all cases. TF: term frequency. None: no decay. Frac.: fractional decay. Exp.: exponential decay. Log.: logarithmic decay. These word distance decay functions are described in detail in this section.}

\end{tabular}
\end{table}
\endgroup

% Footnote symbols:
% https://tex.stackexchange.com/questions/826/symbols-instead-of-numbers-as-footnote-markers

% 1   asterisk        *   2   dagger      †   3   double dagger       ‡
% 4   section symbol  §   5   paragraph   ¶   6   parallel lines      ‖
% 7   two asterisks   **  8   two daggers ††  9   two double daggers  ‡‡

% https://graphicdesign.stackexchange.com/questions/133098/what-is-the-convention-to-indicate-the-same-value-as-above
% https://en.wikipedia.org/wiki/Ditto_mark

% https://tex.stackexchange.com/questions/11366/how-can-i-get-the-figures-not-to-be-pushed-to-the-end-of-the-document
